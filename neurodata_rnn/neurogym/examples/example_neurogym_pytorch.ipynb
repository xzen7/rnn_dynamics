{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch supervised learning of perceptual decision making task\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neurogym/neurogym/blob/master/examples/example_neurogym_pytorch.ipynb)\n",
    "\n",
    "Pytorch-based example code for training a RNN on a perceptual decision-making task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation when used on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gym\n",
    "! pip install gym\n",
    "# Install neurogym\n",
    "! git clone https://github.com/gyyang/neurogym.git\n",
    "%cd neurogym/\n",
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xzheng321\\AppData\\Local\\anaconda3\\envs\\neurogym\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:196: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  f\"The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `{type(result)}`\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import neurogym as ngym\n",
    "\n",
    "# Environment\n",
    "task = 'PerceptualDecisionMaking-v0'\n",
    "kwargs = {'dt': 100}\n",
    "seq_len = 100\n",
    "\n",
    "# Make supervised dataset\n",
    "dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n",
    "                       seq_len=seq_len)\n",
    "env = dataset.env\n",
    "ob_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 loss: 0.09157\n",
      "400 loss: 0.01893\n",
      "600 loss: 0.01281\n",
      "800 loss: 0.01198\n",
      "1000 loss: 0.01114\n",
      "1200 loss: 0.01116\n",
      "1400 loss: 0.01104\n",
      "1600 loss: 0.01079\n",
      "1800 loss: 0.01112\n",
      "2000 loss: 0.01062\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_h):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(ob_size, num_h)\n",
    "        self.linear = nn.Linear(num_h, act_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.lstm(x)\n",
    "        x = self.linear(out)\n",
    "        return x\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Net(num_h=64).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "running_loss = 0.0\n",
    "for i in range(2000):\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n",
    "    labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss = criterion(outputs.view(-1, act_size), labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if i % 200 == 199:\n",
    "        print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (lstm): LSTM(3, 64)\n",
      "  (linear): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance in 200 trials\n",
      "0.895\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make this into a function in neurogym\n",
    "perf = 0\n",
    "num_trial = 200\n",
    "for i in range(num_trial):\n",
    "    env.new_trial()\n",
    "    ob, gt = env.ob, env.gt\n",
    "    ob = ob[:, np.newaxis, :]  # Add batch axis\n",
    "    inputs = torch.from_numpy(ob).type(torch.float).to(device)\n",
    "\n",
    "    action_pred = net(inputs)\n",
    "    action_pred = action_pred.detach().numpy()\n",
    "    action_pred = np.argmax(action_pred, axis=-1)\n",
    "    perf += gt[-1] == action_pred[-1, 0]\n",
    "\n",
    "perf /= num_trial\n",
    "print('Average performance in {:d} trials'.format(num_trial))\n",
    "print(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5.0934e-03, -1.2046e-01, -1.5882e-01,  ..., -2.0963e-01,\n",
       "           -1.7633e-01,  1.0578e-01]],\n",
       " \n",
       "         [[ 2.4956e-02, -1.5824e-01,  1.9214e-04,  ..., -3.2610e-01,\n",
       "           -1.2877e-01,  1.7506e-01]],\n",
       " \n",
       "         [[ 2.5560e-02, -1.2214e-01,  1.7761e-04,  ..., -1.8526e-01,\n",
       "           -4.3815e-02,  1.2272e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.6339e-02, -6.6062e-02,  3.5459e-04,  ..., -1.2552e-01,\n",
       "           -3.7296e-02,  9.0264e-02]],\n",
       " \n",
       "         [[ 2.6406e-02, -6.6035e-02,  1.6718e-04,  ..., -1.2250e-01,\n",
       "           -2.6784e-02,  8.2449e-02]],\n",
       " \n",
       "         [[ 5.4010e-01, -2.1344e-02,  4.8640e-05,  ..., -9.8092e-03,\n",
       "           -7.3958e-03,  2.3478e-02]]], grad_fn=<StackBackward0>),\n",
       " (tensor([[[ 5.4010e-01, -2.1344e-02,  4.8640e-05,  5.5331e-03,  5.8322e-02,\n",
       "            -2.6924e-02,  3.5993e-02,  6.7515e-01, -6.8902e-02,  7.8153e-04,\n",
       "             7.8588e-01, -1.3600e-03,  1.5669e-04, -8.3393e-02, -9.6789e-02,\n",
       "             1.9087e-02, -7.0471e-03, -6.5499e-03,  5.5878e-01,  6.3683e-01,\n",
       "            -2.2722e-01, -9.9640e-01,  2.7389e-04,  9.6156e-04, -5.2999e-06,\n",
       "            -7.6920e-01, -3.3800e-03, -6.5945e-01,  3.2623e-02, -4.2472e-03,\n",
       "             9.3153e-03,  2.6789e-02,  1.4452e-02,  5.2346e-03,  9.8678e-01,\n",
       "             3.8655e-03,  8.9891e-03, -1.8208e-01,  1.2349e-02, -1.9095e-01,\n",
       "            -2.1646e-02, -1.5367e-04,  1.4985e-01,  5.1626e-02, -6.7485e-01,\n",
       "            -1.3879e-02,  7.4069e-02,  6.7754e-03,  5.1970e-02, -2.0392e-05,\n",
       "             1.4883e-02,  6.0799e-01,  8.2756e-04, -5.1959e-01, -3.4769e-02,\n",
       "            -1.5614e-02,  1.1394e-03, -1.2423e-03,  1.2353e-02, -4.6271e-01,\n",
       "            -2.2472e-02, -9.8092e-03, -7.3958e-03,  2.3478e-02]]],\n",
       "         grad_fn=<StackBackward0>),\n",
       "  tensor([[[ 9.1577e-01, -1.0229e-01,  3.2390e-03,  2.0762e-02,  1.9373e+00,\n",
       "            -3.2246e-01,  4.1435e-01,  6.0511e+00, -1.3107e+00,  1.3111e-02,\n",
       "             4.8866e+00, -1.7141e-02,  2.5447e-03, -1.0874e+00, -4.6081e-01,\n",
       "             7.4771e-01, -1.6509e+00, -3.6580e+00,  1.0070e+01,  7.5432e-01,\n",
       "            -2.5940e+00, -5.4786e+00,  9.2412e-03,  2.1940e-02, -8.4451e-04,\n",
       "            -3.7710e+00, -4.0664e-01, -4.5373e+00,  1.4565e+00, -3.5118e-01,\n",
       "             2.8582e-01,  1.6151e+00,  5.0618e-01,  1.5119e-01,  3.1996e+00,\n",
       "             3.4607e-02,  5.9053e-01, -3.8224e+00,  4.9850e-01, -3.3376e+00,\n",
       "            -2.7827e+00, -3.1996e-03,  1.7487e+00,  6.1999e-01, -7.4343e+00,\n",
       "            -2.1119e+00,  3.8996e+00,  7.1459e-02,  5.8241e-01, -8.7767e-04,\n",
       "             5.7914e-01,  7.2621e-01,  9.8393e-03, -5.8685e-01, -4.0298e+00,\n",
       "            -2.5409e-01,  1.6918e-02, -1.2929e-02,  1.4938e+00, -2.6365e+00,\n",
       "            -3.2213e+00, -8.9218e-02, -2.7190e+00,  4.5499e-01]]],\n",
       "         grad_fn=<StackBackward0>)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the lstm hidden state from net\n",
    "net = net.to('cpu')\n",
    "hidden = net.lstm(inputs)\n",
    "hidden "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
